elt-webscraping-and-bigquery
│
├── config
│   ├── venv
│   ├── requirements.txt
│   ├── schedule.bat
│
├── data
│   ├── raw           # Dados brutos extraídos (sales.csv e outros dados webscraped)
│   ├── processed     # Dados tratados (pronto para modelagem ou carregamento)
│   ├── sales.csv
│
├── docs
│   ├── img
│       ├── fluxograma.jpg
│
├── scripts
│   ├── extraction.py  # Script de web scraping para coletar dados
│   ├── processing.py  # Script para tratamento de dados
│   ├── load_to_bigquery.py  # Script de carregamento para o GCP/BigQuery
│
└── README.md


Ordem sugerida para as etapas:
Extração (Extraction): O script extraction.py realiza o web scraping e armazena os dados brutos na pasta data/raw.

Tratamento (Transformation): O script processing.py limpa e trata os dados, salvando os resultados na pasta data/processed. Essa etapa geralmente envolve:

Limpeza de dados (remover valores nulos, corrigir formatos)
Transformação de colunas
Enriquecimento de dados, se necessário.
Modelagem (opcional antes do carregamento): Dependendo da complexidade dos dados, você pode modelar as tabelas já na fase de transformação, criando as tabelas normalizadas ou agregadas.

Carregamento (Load): O script load_to_bigquery.py envia os dados tratados e/ou modelados para o GCP/BigQuery.

Fluxo ETL:
Web Scraping → Data/raw
Tratamento de Dados → Data/processed
Carregamento no BigQuery → GCP/BigQuery
Se você for adicionar scripts para modelagem de tabelas no BigQuery, pode criar uma pasta separada como scripts/modeling.py para essa finalidade.